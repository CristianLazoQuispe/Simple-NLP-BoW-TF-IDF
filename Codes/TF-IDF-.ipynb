{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Src/')\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "filelocation = r'../Data/text1.txt'\n",
    "dataset  = load_text(filelocation)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eget hendrerit elit. Etiam malesuada lacus in sem tempus hendrerit. Pellentesque cursus magna at mi semper, ut consectetur erat vehicula. Etiam augue massa, posuere vitae pharetra vitae, bibendum quis sem. Integer scelerisque a diam vitae cursus. Vivamus quis ex eget leo hendrerit aliquam cursus at leo. Aenean fermentum maximus gravida. Curabitur rhoncus nibh ut quam convallis sagittis ut eget sapien. Ut libero urna, consequat et ex in, pharetra tempus nibh. Nulla mi tellus, bibendum vel consectetur ac, eleifend ut lacus. Pellentesque quis sem felis. Nunc eu tellus tempor, ultrices mi ut, vulputate turpis.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = clean_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lrem ipsum dlr sit am cnsectur ipcg elit dc eg hendrerit elit malua lus sem tempus hendrerit pellentque sus magna mi sem ut cnsectur er veula augue massa psuere ve pharra ve biben qu sem teger scelerque a d ve sus vivamus qu eg le hendrerit ali sus le ae fermentum maximus gravida itur rhncus nibh ut cnvall sagitt ut eg sapien ut liber urna cnsequ pharra tempus nibh nulla mi tellus biben vel cnsectur eleifend ut lus pellentque qu sem fel nunc eu tellus tempr ultric mi ut vulpute turp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words:  128\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lrem', 'ipsum', 'dlr', 'sit', 'am', 'cnsectur', 'ipcg', 'elit', 'dc', 'eg', 'hendrerit', 'malua', 'lus', 'sem', 'tempus', 'pellentque', 'sus', 'magna', 'mi', 'ut', 'er', 'veula', 'augue', 'massa', 'psuere', 've', 'pharra', 'biben', 'qu', 'teger', 'scelerque', 'a', 'd', 'vivamus', 'le', 'ali', 'ae', 'fermentum', 'maximus', 'gravida', 'itur', 'rhncus', 'nibh', 'cnvall', 'sagitt', 'sapien', 'liber', 'urna', 'cnsequ', 'nulla', 'tellus', 'vel', 'eleifend', 'fel', 'nunc', 'eu', 'tempr', 'ultric', 'vulpute', 'turp', 'maecenas', 'trtr', 'rus', 'c', 'prta', 'ni', 'id', 'iul', 'trtique', 'fusce', 'prium', 'rci', 'mt', 'fil', 'mrbi', 'frgilla', 'phasellus', 'lare', 'auctr', 'maur', 'feugi', 'dapibus', 'vtibulum', 'te', 'nullam', 'rutrum', 'dictum', 'lectus', 'tcidunt', 'que', 'cm', 'just', 'praent', 'mus', 'sed', 'vlutp', 'bldit', 'cngue', 'purus', 'cras', 'cndimentum', 'lia', 'dignsim', 'di', 'efficitur', 'nl', 'ter', 'ligula', 'fam', 'prim', 'faucibus', 'elementum', 'suscipit', 'ullamcr', 'mltie', 'fili', 'lbrt', 'pr', 'du', 'ers', 'arcu', 'eumd', 'egtas', 'ven', 'suspendse', 'imdi', 's', 'sdal']\n"
     ]
    }
   ],
   "source": [
    "print([*vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF) Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_TF(text,vocabulary):\n",
    "    encoding = vocabulary.copy()\n",
    "    n = len(text.split(' '))\n",
    "    for word in text.split(' '):\n",
    "        encoding[word]+=1\n",
    "\n",
    "    list_encoding = []\n",
    "    for value in encoding:\n",
    "        list_encoding.append(encoding[value])\n",
    "    \n",
    "    list_encoding = np.array(list_encoding)/n\n",
    "\n",
    "    return list_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_TF = []\n",
    "for text in new_dataset:\n",
    "    encodings_TF.append(encoding_TF(text,vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrem ipsum dlr sit am cnsectur ipcg elit dc eg hendrerit elit malua lus sem tempus hendrerit pellentque sus magna mi sem ut cnsectur er veula augue massa psuere ve pharra ve biben qu sem teger scelerque a d ve sus vivamus qu eg le hendrerit ali sus le ae fermentum maximus gravida itur rhncus nibh ut cnvall sagitt ut eg sapien ut liber urna cnsequ pharra tempus nibh nulla mi tellus biben vel cnsectur eleifend ut lus pellentque qu sem fel nunc eu tellus tempr ultric mi ut vulpute turp\n"
     ]
    }
   ],
   "source": [
    "print(new_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01098901 0.01098901 0.01098901 0.01098901 0.01098901 0.03296703\n",
      " 0.01098901 0.02197802 0.01098901 0.03296703 0.03296703 0.01098901\n",
      " 0.02197802 0.04395604 0.02197802 0.02197802 0.03296703 0.01098901\n",
      " 0.03296703 0.06593407 0.01098901 0.01098901 0.01098901 0.01098901\n",
      " 0.01098901 0.03296703 0.02197802 0.02197802 0.03296703 0.01098901\n",
      " 0.01098901 0.01098901 0.01098901 0.01098901 0.02197802 0.01098901\n",
      " 0.01098901 0.01098901 0.01098901 0.01098901 0.01098901 0.01098901\n",
      " 0.02197802 0.01098901 0.01098901 0.01098901 0.01098901 0.01098901\n",
      " 0.01098901 0.01098901 0.02197802 0.01098901 0.01098901 0.01098901\n",
      " 0.01098901 0.01098901 0.01098901 0.01098901 0.01098901 0.01098901\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(encodings_TF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_IDF(dataset,vocabulary):\n",
    "    n = len(dataset)\n",
    "    \n",
    "    encoding = vocabulary.copy()\n",
    "    print('number of sentences :',n)\n",
    "    for word in vocabulary:\n",
    "        for text in dataset:\n",
    "            if word in text.split(' '):\n",
    "                encoding[word]+=1\n",
    "    \n",
    "    list_encoding = []\n",
    "    for value in encoding:\n",
    "        list_encoding.append(encoding[value])\n",
    "    \n",
    "    list_encoding = np.log(n/np.array(list_encoding))\n",
    "\n",
    "    return list_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 5\n"
     ]
    }
   ],
   "source": [
    "encodings_IDF = encoding_IDF(new_dataset,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrem ipsum dlr sit am cnsectur ipcg elit dc eg hendrerit elit malua lus sem tempus hendrerit pellentque sus magna mi sem ut cnsectur er veula augue massa psuere ve pharra ve biben qu sem teger scelerque a d ve sus vivamus qu eg le hendrerit ali sus le ae fermentum maximus gravida itur rhncus nibh ut cnvall sagitt ut eg sapien ut liber urna cnsequ pharra tempus nibh nulla mi tellus biben vel cnsectur eleifend ut lus pellentque qu sem fel nunc eu tellus tempr ultric mi ut vulpute turp\n"
     ]
    }
   ],
   "source": [
    "print(new_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91629073 0.22314355 0.91629073 0.22314355 0.22314355 0.91629073\n",
      " 1.60943791 0.91629073 0.51082562 0.51082562 0.91629073 0.\n",
      " 0.         0.22314355 0.91629073 0.91629073 0.51082562 1.60943791\n",
      " 0.91629073 0.22314355 0.22314355 0.51082562 0.22314355 0.51082562\n",
      " 0.51082562 0.51082562 0.91629073 1.60943791 0.22314355 0.91629073\n",
      " 0.91629073 0.51082562 0.91629073 1.60943791 0.51082562 0.\n",
      " 0.51082562 1.60943791 0.51082562 0.51082562 1.60943791 0.51082562\n",
      " 0.51082562 0.91629073 0.51082562 0.22314355 1.60943791 1.60943791\n",
      " 0.91629073 0.22314355 0.51082562 0.         1.60943791 0.51082562\n",
      " 0.22314355 0.91629073 0.91629073 1.60943791 1.60943791 0.91629073\n",
      " 0.91629073 0.91629073 1.60943791 0.22314355 0.91629073 0.91629073\n",
      " 0.51082562 1.60943791 1.60943791 1.60943791 0.91629073 1.60943791\n",
      " 0.91629073 0.51082562 0.91629073 0.91629073 0.91629073 1.60943791\n",
      " 0.51082562 1.60943791 1.60943791 0.91629073 0.22314355 0.51082562\n",
      " 0.51082562 0.91629073 0.91629073 0.91629073 0.51082562 0.91629073\n",
      " 1.60943791 0.51082562 1.60943791 1.60943791 0.51082562 0.91629073\n",
      " 1.60943791 0.91629073 1.60943791 1.60943791 1.60943791 0.91629073\n",
      " 1.60943791 0.91629073 1.60943791 0.91629073 0.91629073 1.60943791\n",
      " 0.91629073 0.91629073 0.91629073 0.91629073 1.60943791 1.60943791\n",
      " 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791\n",
      " 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791 1.60943791\n",
      " 1.60943791 1.60943791]\n"
     ]
    }
   ],
   "source": [
    "print(encodings_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_TF_IDF = []\n",
    "for idx,text in enumerate(new_dataset):\n",
    "    encodings_TF_IDF.append(encodings_TF[idx]*encodings_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lrem ipsum dlr sit am cnsectur ipcg elit dc eg hendrerit elit malua lus sem tempus hendrerit pellentque sus magna mi sem ut cnsectur er veula augue massa psuere ve pharra ve biben qu sem teger scelerque a d ve sus vivamus qu eg le hendrerit ali sus le ae fermentum maximus gravida itur rhncus nibh ut cnvall sagitt ut eg sapien ut liber urna cnsequ pharra tempus nibh nulla mi tellus biben vel cnsectur eleifend ut lus pellentque qu sem fel nunc eu tellus tempr ultric mi ut vulpute turp'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01006913, 0.00245213, 0.01006913, 0.00245213, 0.00245213,\n",
       "       0.03020739, 0.01768613, 0.02013826, 0.00561347, 0.01684041,\n",
       "       0.03020739, 0.        , 0.        , 0.00980851, 0.02013826,\n",
       "       0.02013826, 0.01684041, 0.01768613, 0.03020739, 0.01471276,\n",
       "       0.00245213, 0.00561347, 0.00245213, 0.00561347, 0.00561347,\n",
       "       0.01684041, 0.02013826, 0.03537226, 0.00735638, 0.01006913,\n",
       "       0.01006913, 0.00561347, 0.01006913, 0.01768613, 0.01122694,\n",
       "       0.        , 0.00561347, 0.01768613, 0.00561347, 0.00561347,\n",
       "       0.01768613, 0.00561347, 0.01122694, 0.01006913, 0.00561347,\n",
       "       0.00245213, 0.01768613, 0.01768613, 0.01006913, 0.00245213,\n",
       "       0.01122694, 0.        , 0.01768613, 0.00561347, 0.00245213,\n",
       "       0.01006913, 0.01006913, 0.01768613, 0.01768613, 0.01006913,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_TF_IDF[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../Results/tf-idf-encoding.npy', encodings_TF_IDF) \n",
    "encodings_TF_IDF = np.load('../Results/tf-idf-encoding.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
